# 2025-ICLR-Dynamic-LLaVA- Efficient Multimodal Large Language Models via Dynamic Vision-Language Context Sparsification

## 背景

- 多模态大型语言模型虽然在视觉理解、推理和交互方面表现出色，但其推理阶段随着输出文本令牌数量增加，计算和内存消耗也呈递增趋势，尤其是在解码阶段。这种增长严重影响了MLLMs的推理效率。
- 现有的高效推理方法主要集中在预填充阶段减少视觉信息冗余，如减少图像令牌，但这种预填充阶段的效率提升会随着解码过程的进行而逐渐减弱，因为解码阶段的计算瓶颈逐渐转向了自回归生成的语言令牌。
- 传统的KV缓存压缩方法虽然在一定程度上减轻了GPU内存压力，但通常依赖历史KV缓存的选择，且无法同时提升预填充和无KV缓存解码阶段的效率，对多模态场景适应性差。

![image-20250428232442550](/Users/iamzlt/Documents/paper/KnowledgeDistillation-VisualPerception/images/image-20250428232442550.png)

> 该图显示，尽管视觉令牌的稀疏化在预填充阶段能显著节省资源，但随着更多输出文本令牌的生成，计算和内存开销仍然逐渐增加，且传统方法在解码阶段的效率优势减少。这一现象说明了仅仅减少图像令牌数量不能持续提升解码阶段的推理效率

## 目的

- 提出Dynamic-LLaVA，一个动态视听语言上下文稀疏化框架，旨在通过对视觉上下文和语言上下文的动态稀疏化，提升MLLMs在推理全过程中的计算和内存效率。
- 在预填充和解码阶段分别设计适配不同推理模式（预填充、无KV缓存解码、有KV缓存解码）的稀疏化策略，实现对视觉和语言令牌的动态选择和过滤，持续减少计算量和内存占用。
- 通过端到端训练使模型能够动态学习并调整保留的重要令牌，从而在保证理解和生成能力的前提下，大幅度降低资源消耗。
- 实验中，Dynamic-LLaVA能在预填充阶段减少约75%的计算，在解码阶段减少约50%的计算和GPU内存使用，同时维持甚至提升模型性能，具备实际应用潜力。

总结来说，论文旨在解决多模态大型语言模型在解码过程中的推理效率瓶颈，提出动态且针对不同阶段和模式的上下文稀疏化方法，实现高效推理同时保证模型性能。
