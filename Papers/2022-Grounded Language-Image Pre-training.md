# 2022-Grounded Language-Image Pre-training

这篇论文提出了一种名为GLIP（Grounded Language-Image Pre-training）的视觉预训练模型，旨在同时学习具备语言理解能力的对象级视觉表示。作者通过将目标检测任务和短语定位任务进行统一建模，使模型不仅能够识别图像中的目标，还能理解文本短语与图像区域之间的语义对应关系。GLIP通过构建一个联合的图像-文本编码框架，实现了视觉特征和语言特征的深度融合，从而显著增强了模型的语义理解能力。该模型在大规模数据集上进行预训练，包括人工标注和从网络爬取的图文对，通过自训练方式自动生成定位框，实现了高效的数据扩展。在实验中，GLIP在COCO、LVIS等标准数据集上展现出优异的零样本和少样本迁移能力，并在13个实际应用场景下取得了与监督模型相当甚至更优的性能，验证了其强大的通用性和部署灵活性。这项研究展示了多模态预训练在视觉理解中的巨大潜力，为构建更泛化、低成本的目标检测系统提供了新路径。

## 摘要

本文提出了一种面向对象级别、具备语言感知能力并富含语义信息的视觉表示学习模型——GLIP（Grounded Language-Image Pre-training）。该模型将目标检测与短语定位任务统一起来进行预训练。这一统一带来了两大优势：

1. GLIP可同时从检测数据和定位数据中学习，从而提升两个任务的性能，并构建出优质的定位模型；
2. GLIP能够通过自训练方式为大量图文对生成定位框，从而学习到语义丰富的表示。

在实验中，GLIP在2700万条定位数据上进行了预训练，其中包括300万条人工标注数据和2400万条网络爬取的图文对。实验结果表明，GLIP在多个对象级识别任务中展现出强大的零样本和少样本迁移能力。

1. 在未使用COCO图像进行预训练的情况下，GLIP在COCO和LVIS数据集上直接评估时分别获得了49.8和26.9的AP，超过了许多监督学习基线；
2. 在COCO上进行微调后，GLIP在验证集和test-dev上分别达到了60.8和61.5的AP，超越了以往的SOTA方法；
3. 在迁移至13个下游目标检测任务时，仅使用1个样本的GLIP就可媲美完全监督的Dynamic Head模型。代码将公开于：https://github.com/microsoft/GLIP。