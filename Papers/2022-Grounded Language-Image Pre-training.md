# 2022-Grounded Language-Image Pre-training

这篇论文提出了一种名为GLIP（Grounded Language-Image Pre-training）的视觉预训练模型，旨在同时学习具备语言理解能力的对象级视觉表示。作者通过将目标检测任务和短语定位任务进行统一建模，使模型不仅能够识别图像中的目标，还能理解文本短语与图像区域之间的语义对应关系。GLIP通过构建一个联合的图像-文本编码框架，实现了视觉特征和语言特征的深度融合，从而显著增强了模型的语义理解能力。该模型在大规模数据集上进行预训练，包括人工标注和从网络爬取的图文对，通过自训练方式自动生成定位框，实现了高效的数据扩展。在实验中，GLIP在COCO、LVIS等标准数据集上展现出优异的零样本和少样本迁移能力，并在13个实际应用场景下取得了与监督模型相当甚至更优的性能，验证了其强大的通用性和部署灵活性。这项研究展示了多模态预训练在视觉理解中的巨大潜力，为构建更泛化、低成本的目标检测系统提供了新路径。

## 摘要

本文提出了一种面向对象级别、具备语言感知能力并富含语义信息的视觉表示学习模型——GLIP（Grounded Language-Image Pre-training）。该模型将目标检测与短语定位任务统一起来进行预训练。这一统一带来了两大优势：

1. GLIP可同时从检测数据和定位数据中学习，从而提升两个任务的性能，并构建出优质的定位模型；
2. GLIP能够通过自训练方式为大量图文对生成定位框，从而学习到语义丰富的表示。

在实验中，GLIP在2700万条定位数据上进行了预训练，其中包括300万条人工标注数据和2400万条网络爬取的图文对。实验结果表明，GLIP在多个对象级识别任务中展现出强大的零样本和少样本迁移能力。

1. 在未使用COCO图像进行预训练的情况下，GLIP在COCO和LVIS数据集上直接评估时分别获得了49.8和26.9的AP，超过了许多监督学习基线；
2. 在COCO上进行微调后，GLIP在验证集和test-dev上分别达到了60.8和61.5的AP，超越了以往的SOTA方法；
3. 在迁移至13个下游目标检测任务时，仅使用1个样本的GLIP就可媲美完全监督的Dynamic Head模型。代码将公开于：https://github.com/microsoft/GLIP。

# 📘 Introduction

视觉识别模型通常被训练用于预测一组预先定义好的固定对象类别，这限制了它们在现实应用中的实用性，因为要泛化到新的视觉概念和领域，往往还需要额外的人工标注数据。CLIP 的研究表明，通过大量原始图文对，可以有效地学习图像级的视觉表示。由于这些配对的文本通常包含比任何预设类别集合都更广泛的视觉概念，CLIP 预训练模型具备非常丰富的语义能力，因此在零样本设置下，能够轻松迁移到图像分类和图文检索等下游任务中。

然而，为了实现对图像更细粒度的理解——这是许多任务所必需的，比如目标检测、图像分割、人体姿态估计、场景理解、行为识别以及视觉语言理解等任务——我们迫切需要对象级的视觉表示。在本研究中，我们提出“短语定位”（phrase grounding）任务作为一种高效且可扩展的预训练手段，用于学习对象级、具备语言感知能力、并富含语义的视觉表示，并据此提出GLIP（Grounded Language-Image Pre-training）模型。我们的方法统一了短语定位任务与目标检测任务：目标检测可以看作是“无上下文”的短语定位，而短语定位可以看作是“有上下文”的目标检测。我们通过将目标检测重构为短语定位，实现了检测与定位任务的统一。

**这种重构改变了检测模型的输入形式：模型的输入不仅包括图像，还包括一个文本提示，用于描述该检测任务中的所有候选类别。**例如，在 COCO 目标检测任务中，文本提示由 80 个类别名称组成的字符串构成，这 80 个类别以句号连接，如图1左侧所示。任何目标检测模型都可以通过将其框分类器中的分类 logits 替换为“词语-区域对齐得分”而转化为短语定位模型。这里的对齐得分指的是区域（或边框）的视觉特征与词语（或短语）的语言特征之间的点积，如图1右侧所示。语言特征由语言模型计算得出，这使得新的检测（或定位）模型具有一个双编码器结构。与 CLIP 不同，CLIP 仅在最后的点积层融合视觉与语言信息，而我们提出的 GLIP 则在模型中部引入了深层的跨模态融合（如图1中部所示），这一点对于学习高质量、具备语言感知能力的视觉表示以及实现更优越的迁移学习性能至关重要。检测与定位任务的统一还使我们能够同时利用这两类数据进行预训练，并且这种方式对两个任务都是有益的。对于目标检测而言，得益于定位数据的引入，视觉概念的种类得到了显著扩展；而对于短语定位而言，目标检测数据提供了更多的边框注释，有助于训练出新的最先进（SoTA）的短语定位模型。

**利用海量图文数据扩展视觉概念。** 一旦拥有了一个性能良好的定位模型（即教师模型），我们便可以通过自动生成定位框的方式，为大量图文对扩充GLIP的预训练数据。这些图文对中的名词短语由自然语言处理工具（NLP parser）识别得到【参考文献2】。因此，我们可以在2700万条定位数据上对学生模型GLIP-L（GLIP-Large）进行预训练，其中包括300万条人工标注的细粒度数据和2400万条从网络爬取的图文对。在这2400万条图文对中，我们共生成了7810万个置信度大于0.5的伪定位框注释，涉及5840万个独特的名词短语。我们在图2中展示了两个由教师模型生成的定位框示例。教师模型可以准确地定位一些具有挑战性的概念，例如“注射器”“疫苗”“美丽的加勒比海蓝绿色”，甚至包括抽象词语（如“风景”）。在如此语义丰富的数据上进行训练，能够得到同样语义丰富的学生模型。相比之下，先前的检测数据扩展方法无法预测超出教师模型预定义词汇表的概念【参考文献68】。而在本研究中，我们表明，通过扩大短语定位数据的规模这一简单策略，在实际中是非常有效的，尤其在 LVIS 和 13 个下游目标检测任务上带来了显著提升，特别是在识别稀有类别方面（详见第4.2节和第5节）。当预训练完成的 GLIP-L 模型在 COCO 数据集上进行微调后，在 COCO 2017 的验证集上取得了 60.8 的 AP，在 test-dev 上达到 61.5，超越了当前通过各种方法扩展目标检测数据而得到的公开最先进模型【参考文献9，58】。

**GLIP 的迁移学习能力：一模型通用多任务。** 通过定位重构和语义丰富的预训练，GLIP 极大地提升了跨领域的迁移能力。它可以在仅使用极少甚至完全不使用人工标注的情况下，迁移到多种任务上。当我们在未使用任何 COCO 图像进行预训练的前提下，直接在 COCO 和 LVIS 数据集上评估 GLIP-L 模型时，其在 COCO val2017 上取得了 49.8 的 AP，在 LVIS val 上达到 26.9 的 AP，均超越了许多监督学习的基线模型。当在13个已有的目标检测数据集上进行评估时（这些数据集涵盖了细粒度物种检测、无人机视角检测、第一人称视角检测等多种场景，我们称之为“真实世界目标检测任务”即 ODinW，详见第5.1节），GLIP 展现出了卓越的数据效率。例如，在零样本设置下，GLIP-L 的表现超过了在 Objects365 上预训练并使用10个样本的监督基线模型（Dynamic Head）；而在仅使用1个样本的情况下，GLIP-L 的性能也可以与完全监督的 Dynamic Head 相媲美。此外，当任务的特定标注数据可用时，我们无需微调整个模型，仅需调整任务相关的提示词嵌入（prompt embedding），即可保持模型参数不变地适应新任务。在这种提示词微调（prompt tuning）的设置下（详见第5.2节），一个 GLIP 模型就可以同时在所有下游任务中表现良好，从而显著降低微调与部署成本。







